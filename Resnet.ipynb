{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yakovsushenok/Thesis/blob/main/Resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E74CWhj4Cd-n",
        "outputId": "aef629d9-1cc5-49ad-a273-e4af6209a9c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc2b7368ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import math, random\n",
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "drive.mount('/content/gdrive')\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy.matlib\n",
        "try:\n",
        "    from scipy.fftpack import fft, ifft\n",
        "except ImportError:\n",
        "    from numpy.fft import fft, ifft\n",
        "from scipy.signal import lfilter\n",
        "import scipy.io as sio\n",
        "from scipy import signal\n",
        "import gc\n",
        "import h5py\n",
        "from torchsummary import summary\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_500 = pd.read_csv(\"/content/gdrive/MyDrive/train_metadata_more_than_500.csv\")\n",
        "df_500['primary_label'] = df_500['primary_label'].apply(lambda x: x - 1)\n",
        "df_500 = df_500[['relative_path', 'primary_label']]\n",
        "df_500_toy = pd.read_csv(\"/content/gdrive/MyDrive/df_500_toy.csv\")\n",
        "toy_ind = list(df_500_toy['Unnamed: 0'])\n",
        "\n",
        "class H5DS(Dataset):\n",
        "  def __init__(self, df, path, toy_ind):\n",
        "    self.path = path\n",
        "    self.data = h5py.File(self.path, 'r')['data']\n",
        "    self.df = df\n",
        "    self.toy_ind = toy_ind\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.df)    \n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "   \n",
        "   return (self.data[toy_ind[idx]], torch.tensor(self.df['primary_label'].iloc[idx]))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else  \"cpu\") #\n",
        "\n",
        "def inference(model, val_dl):\n",
        "  correct_prediction = 0\n",
        "  total_prediction = 0\n",
        "\n",
        "  # Disable gradient updates\n",
        "  with torch.no_grad():\n",
        "    for data in val_dl:\n",
        "      # Get the input features and target labels, and put them on the GPU\n",
        "      inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "      # Normalize the inputs\n",
        "      # inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "      # inputs = (7 + (inputs - inputs_m) / inputs_s)\n",
        "\n",
        "      inputs -= inputs.min(1, keepdim=True)[0]\n",
        "      inputs /= inputs.max(1, keepdim=True)[0]\n",
        "\n",
        "      inputs = inputs[None, :, :, :]\n",
        "      inputs = inputs.permute(1, 0, 2, 3)\n",
        "      inputs = torch.cat([inputs[:, :, :round(inputs.shape[2]/3), :],inputs[:, :, round(inputs.shape[2]/3):round(inputs.shape[2]*2/3), :],inputs[:, :, round(inputs.shape[2]*2/3):, : ]], dim = 1)\n",
        "\n",
        "      # Get predictions\n",
        "      outputs = model(inputs.float())\n",
        "\n",
        "      # Get the predicted class with the highest score\n",
        "      _, prediction = torch.max(outputs,1)\n",
        "      # Count of predictions that matched the target label\n",
        "      correct_prediction += (prediction == labels).sum().item()\n",
        "      total_prediction += prediction.shape[0]\n",
        "    \n",
        "  acc = correct_prediction/total_prediction\n",
        "  print(f'Val Accuracy: {acc:.2f}')"
      ],
      "metadata": {
        "id": "QzkT_yLbhHRL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet Model\n",
        "\n",
        "[Link](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) for saving best model (how to do it)"
      ],
      "metadata": {
        "id": "anhyzMFGVIMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample):\n",
        "        super().__init__()\n",
        "        if downsample:\n",
        "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "            self.shortcut = nn.Sequential()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, input):\n",
        "        shortcut = self.shortcut(input)\n",
        "        input = nn.ReLU()(self.bn1(self.conv1(input)))\n",
        "        input = nn.ReLU()(self.bn2(self.conv2(input)))\n",
        "        input = input + shortcut\n",
        "        return nn.ReLU()(input)\n",
        "\n",
        "class ResBottleneckBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels//4, kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels//4, out_channels//4, kernel_size=3, stride=2 if downsample else 1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(out_channels//4, out_channels, kernel_size=1, stride=1)\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if self.downsample or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2 if self.downsample else 1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels//4)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels//4)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, input):\n",
        "        shortcut = self.shortcut(input)\n",
        "        input = nn.ReLU()(self.bn1(self.conv1(input)))\n",
        "        input = nn.ReLU()(self.bn2(self.conv2(input)))\n",
        "        input = nn.ReLU()(self.bn3(self.conv3(input)))\n",
        "        input = input + shortcut\n",
        "        return nn.ReLU()(input)\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels, resblock, repeat, useBottleneck=False, outputs=12):\n",
        "        super().__init__()\n",
        "        self.layer0 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        if useBottleneck:\n",
        "            filters = [64, 256, 512, 1024, 2048]\n",
        "        else:\n",
        "            filters = [64, 64, 128, 256, 512]\n",
        "\n",
        "        self.layer1 = nn.Sequential()\n",
        "        self.layer1.add_module('conv2_1', resblock(filters[0], filters[1], downsample=False))\n",
        "        for i in range(1, repeat[0]):\n",
        "                self.layer1.add_module('conv2_%d'%(i+1,), resblock(filters[1], filters[1], downsample=False))\n",
        "\n",
        "        self.layer2 = nn.Sequential()\n",
        "        self.layer2.add_module('conv3_1', resblock(filters[1], filters[2], downsample=True))\n",
        "        for i in range(1, repeat[1]):\n",
        "                self.layer2.add_module('conv3_%d' % (i+1,), resblock(filters[2], filters[2], downsample=False))\n",
        "\n",
        "        self.layer3 = nn.Sequential()\n",
        "        self.layer3.add_module('conv4_1', resblock(filters[2], filters[3], downsample=True))\n",
        "        for i in range(1, repeat[2]):\n",
        "            self.layer3.add_module('conv2_%d' % (i+1,), resblock(filters[3], filters[3], downsample=False))\n",
        "\n",
        "        self.layer4 = nn.Sequential()\n",
        "        self.layer4.add_module('conv5_1', resblock(filters[3], filters[4], downsample=True))\n",
        "        for i in range(1, repeat[3]):\n",
        "            self.layer4.add_module('conv3_%d'%(i+1,), resblock(filters[4], filters[4], downsample=False))\n",
        "\n",
        "        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = torch.nn.Linear(filters[4], outputs)\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = self.layer0(input)\n",
        "        input = self.layer1(input)\n",
        "        input = self.layer2(input)\n",
        "        input = self.layer3(input)\n",
        "        input = self.layer4(input)\n",
        "        input = self.gap(input)\n",
        "        input = torch.flatten(input, start_dim=1)\n",
        "        input = self.fc(input)\n",
        "\n",
        "        return input\n",
        "class H5DS(Dataset):\n",
        "  def __init__(self, df, path):\n",
        "    self.path = path\n",
        "    self.data = h5py.File(self.path, 'r')['data']\n",
        "    self.df = df\n",
        "    self.toy_ind = toy_ind\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.df)    \n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "   \n",
        "   return (self.data[idx], torch.tensor(self.df['primary_label'].iloc[idx]))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else  \"cpu\") #\n",
        "\n",
        "#myModel = ResNet(3, ResBlock, [2, 2, 2, 2], useBottleneck=False, outputs=12) # resnet18\n",
        "#myModel = ResNet(3, ResBottleneckBlock, [3, 4, 6, 3], useBottleneck=True, outputs=12) # resnet50\n",
        "myModel = ResNet(3, ResBottleneckBlock, [3, 8, 36, 3], useBottleneck=True, outputs=12)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else  \"cpu\") #\n",
        "myModel = myModel.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def training(model, train_dl, num_epochs, val_dl):\n",
        "  # Loss Function, Optimizer \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr= 10**(-5))\n",
        "  \n",
        "  # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.00001,\n",
        "  #                                               steps_per_epoch=int(len(train_dl)),\n",
        "  #                                               epochs=num_epochs,\n",
        "  #                                               anneal_strategy='linear')\n",
        "\n",
        "  # Epoch iterator\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_prediction = 0\n",
        "    total_prediction = 0\n",
        "    indices = []\n",
        "    # Batch iterator\n",
        "    j = 0\n",
        "    for i, data in enumerate(train_dl):\n",
        "\n",
        "        inputs, labels = torch.tensor(data[0]).to(device), torch.tensor(data[1]).to(device) # Get the input features and target labels, and put them on the GPU\n",
        "        if torch.isnan(torch.tensor(data[0])).any() == True:\n",
        "          j += 1\n",
        "          continue\n",
        "        # Normalize the inputs - 1 (ignore the 7). We normalize the tensors using the mean from the whole dataset, not per-image.https://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current\n",
        "        # inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "        # inputs = (7 + (inputs - inputs_m) / inputs_s)\n",
        "        \n",
        "        # Normalize the inputs - 2\n",
        "        \n",
        "        inputs -= inputs.min(1, keepdim=True)[0]\n",
        "        inputs /= inputs.max(1, keepdim=True)[0]\n",
        "\n",
        "        inputs = inputs[None, :, :, :]\n",
        "        inputs = inputs.permute(1, 0, 2, 3)\n",
        "        inputs = torch.cat([inputs[:, :, :round(inputs.shape[2]/3), :],inputs[:, :, round(inputs.shape[2]/3):round(inputs.shape[2]*2/3), :],inputs[:, :, round(inputs.shape[2]*2/3):, : ]], dim = 1)\n",
        "\n",
        "        optimizer.zero_grad() # Zero the parameter gradients\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "\n",
        "        running_loss += loss.item() # Keep stats for Loss and Accuracy\n",
        "\n",
        "        _, prediction = torch.max(outputs,1) # Get the predicted class with the highest score\n",
        "        #print(prediction, labels)\n",
        "        correct_prediction += (prediction == labels).sum().item() # Count of predictions that matched the target label\n",
        "        total_prediction += prediction.shape[0]\n",
        "\n",
        "    \n",
        "    # Print stats at the end of the epoch\n",
        "    print(j)\n",
        "    num_batches = len(train_dl)\n",
        "    avg_loss = running_loss / num_batches\n",
        "    acc = correct_prediction/total_prediction\n",
        "    print(f'Epoch: {epoch + 1}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
        "    print(f\"TESTING:\")\n",
        "    inference(model, val_dl)\n",
        "    print(\"\\n\")\n",
        "\n",
        "  print('Finished Training')\n",
        "\n",
        "\n",
        "path = '/content/gdrive/MyDrive/df_train_tensor_60_fs50-8k.h5' #               '/content/gdrive/MyDrive/df_train_tensor_60_500.h5'\n",
        "NUM_EPOCHS = 40\n",
        "# Initializing the dataset\n",
        "myds = H5DS(df_500, path)\n",
        "# Random split of 80:20 between training and validation\n",
        "num_items = len(myds)\n",
        "num_train = round(num_items*(0.80))\n",
        "num_val = num_items - num_train\n",
        "print(num_train,num_val)\n",
        "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
        "# Create training and validation data loaders\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=1, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=1, shuffle=False)\n",
        "\n",
        "training(myModel, train_dl, NUM_EPOCHS, val_dl) # Training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UbzrXt2VKTQ",
        "outputId": "c64401c4-142d-4a9c-e6e6-421f3cfb156b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4800 1200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch: 1, Loss: 2.30, Accuracy: 0.20\n",
            "TESTING:\n",
            "Val Accuracy: 0.31\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 2, Loss: 1.84, Accuracy: 0.38\n",
            "TESTING:\n",
            "Val Accuracy: 0.48\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 3, Loss: 1.46, Accuracy: 0.52\n",
            "TESTING:\n",
            "Val Accuracy: 0.55\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 4, Loss: 1.14, Accuracy: 0.64\n",
            "TESTING:\n",
            "Val Accuracy: 0.63\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 5, Loss: 0.90, Accuracy: 0.72\n",
            "TESTING:\n",
            "Val Accuracy: 0.68\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 6, Loss: 0.73, Accuracy: 0.77\n",
            "TESTING:\n",
            "Val Accuracy: 0.71\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 7, Loss: 0.60, Accuracy: 0.81\n",
            "TESTING:\n",
            "Val Accuracy: 0.69\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 8, Loss: 0.49, Accuracy: 0.85\n",
            "TESTING:\n",
            "Val Accuracy: 0.66\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 9, Loss: 0.43, Accuracy: 0.87\n",
            "TESTING:\n",
            "Val Accuracy: 0.66\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 10, Loss: 0.37, Accuracy: 0.89\n",
            "TESTING:\n",
            "Val Accuracy: 0.72\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 11, Loss: 0.31, Accuracy: 0.90\n",
            "TESTING:\n",
            "Val Accuracy: 0.68\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 12, Loss: 0.30, Accuracy: 0.91\n",
            "TESTING:\n",
            "Val Accuracy: 0.71\n",
            "\n",
            "\n",
            "0\n",
            "Epoch: 13, Loss: 0.25, Accuracy: 0.92\n",
            "TESTING:\n",
            "Val Accuracy: 0.71\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results:\n",
        "\n",
        "---\n",
        "\n",
        "Sample Length (s) : 60\n",
        "\n",
        "lr= 10**(-5)\n",
        "\n",
        "Model: Resnet18\n",
        "\n",
        "Normalization: Yes\n",
        "\n",
        "Number of classes: 12\n",
        "\n",
        "Mini-batch size = 1\n",
        "\n",
        "Number of Samples in training: 2000*(0.8)\n",
        "\n",
        "`fs = [50, 8000]`\n",
        "\n",
        "Best Val Accurary = 0.69 epoch 14/30 $|$ 0.81 epoch 17\n",
        "\n",
        "Input shape: `(768, 6025)`\n",
        "\n",
        "\n",
        "Trainable params: 11,181,000\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Sample Length (s) : 60\n",
        "\n",
        "lr= 10**(-5)\n",
        "\n",
        "Model: Resnet 50\n",
        "\n",
        "Normalization: Yes\n",
        "\n",
        "Number of classes: 12\n",
        "\n",
        "Mini-batch size = 1\n",
        "\n",
        "Number of Samples in training: 2000*(0.8)\n",
        "\n",
        "`fs = [50, 8000]`\n",
        "\n",
        "Best Val Accurary = 0.51 epoch 11/15\n",
        "\n",
        "Input shape: `(768, 6025)`\n",
        "\n",
        "\n",
        "Trainable params: 23,559,180\n"
      ],
      "metadata": {
        "id": "rbFKIrkNS2_a"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Resnet.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMxFFTwadtzuekysQfv+FoV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}